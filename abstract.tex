Autonomous Vehicles (AVs) are almost certainly becoming a reality for the near future thanks to the substantial recent advances in computer vision.
To make this happen, it is of paramount importance to give to AVs the ability to understand the surrounding scene.
This paper presents a system capable to work in a real-time and online fashion, giving an immediate response to the arise of anomalies surrounding the AV exploiting only the videos captured by a dash-mounted camera.
Our proposed architecture, called \emph{MOVAD}, is composed by two modules: a short-term memory to extract information related to the ongoing action, implemented by a Video Swin Trasformer adapted to work in a real-time scenario, and a long-term memory module that considers remote past information thanks to the use of a Long-Short Term Memory (LSTM).
We evaluated the performance of our method on Detection of Traffic Anomaly (DoTA) dataset, a challenging collection of dash-mounted camera videos of accidents.
After an extensive ablation study, we were able to reach an AUC score of \anote{81.69}, surpassing the current state-of-the-art.
Our code will be available on GitHub prior to final submission.