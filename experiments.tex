\section{Experimental Results}
\label{sec:experiments}


\noindent\textbf{Dataset}.
We perform our tests on DoTA dataset \cite{9712446}.
It contains 4677 videos taken from YouTube, recorded in different countries and with different illumination and weather conditions, with a resolution of $1280 \times 720$.
Of all the annotations available, we use only the start and end of the anomaly.
%We use the original dataset split, which approximately divide the training and validation in $70\%$ and $30\%$.
Our tests are related to the Task~1~\cite{9712446}, the (frame-level) VAD.
Our model training and evaluation take place in the online scenario, that is the most realistic and most interesting condition from our point of view.

\noindent\textbf{Evaluation Metrics}.
We use the well-known Area Under the Curve (AUC) metric at frame-level, to evaluate how well the model is able to temporally locate the anomaly in the videos.

\noindent\textbf{Implementation details.}
%The results of the models, with which we compare, are taken from the respective papers.
We perform the training on a single machine with an A100 GPU\@, with Stochastic Gradient Descent (SGD) optimization algorithm, learning rate of 0.0001, momentum of 0.9, video clip length (VCL) 8 and batch size 8.
We use SGD instead of Adam because in our experiments the latter led the training to be more unstable, resulting in model diverging after a few epochs.

\newcommand{\figsize}{0.7\columnwidth}

\begin{figure}[ht!]
\centerline{\includegraphics[clip,width=\figsize]{images/exp_1.jpg}}
	\caption{Performance comparison, changing the number of frames ($\mathit{NF}$) in input to the VST (from 1 to 5).}
	\label{fig:num-frames-vst}
\end{figure}


\begin{figure}[ht!]
\centerline{\includegraphics[clip,width=\figsize]{images/exp_2.jpg}}
    % TODO aggiorna caption!
	\caption{Performance comparison, changing the number of LSTM cells (from 0 to 4). ``LSTM (2 cells)'' corresponds to ``$\mathit{NF}=3$''  in Fig.~\ref{fig:num-frames-vst}.}
	\label{fig:num-memory-cells}
\end{figure}


\begin{figure}[ht!]
\centerline{\includegraphics[clip,width=\figsize]{images/exp_3.jpg}}
    \caption{Performance comparison, changing the VCL (from 4 to 16). The ``8 frames'' configuration corresponds to ``$\mathit{NF}=3$'' in Fig.~\ref{fig:num-frames-vst}.\label{fig:random-batch}}
\end{figure}

\begin{table}[ht!]
	\footnotesize
	%\setlength{\tabcolsep}{1.2pt}
	\centerline{\begin{tabular}{!r|^c|^c|^c|}
			\# & Short-term & Long-term & $AUC$ \\
			\hline\hline
	   	            1 &            &            & 66.53 \\
	   	            2 & \checkmark &            & 74.46 \\
	   	            3 &            & \checkmark & 68.76 \\
\rowstyle{\bfseries}4 & \checkmark & \checkmark & 79.21 \\
\end{tabular}}
	\caption{Performance comparison, with and w/out short and long-term memory for 400 epochs. Short-term: with ($\mathit{NF}=3$) and w/out ($\mathit{NF}=1$). Long-term: with (2 cells) and w/out (0 cells). }
	\label{tab:short-vs-long-term-auc}
\end{table}

\begin{table}[ht!]
	\footnotesize
	%\setlength{\tabcolsep}{1.2pt}
	\centerline{\begin{tabular}{!r|^l|^l|^c|}
			\# & Method & Input & $AUC$ \\
			\hline\hline
	   	            1 & ConvAE \cite{hasan2016learning}       & Gray                   & 64.3 \\
			        2 & ConvAE \cite{hasan2016learning}       & Flow                   & 66.3 \\
                    3 & ConvLSTMAE \cite{chong2017abnormal}   & Gray                   & 53.8 \\
                    4 & ConvLSTMAE \cite{chong2017abnormal}   & Flow                   & 62.5 \\
                    5 & AnoPred \cite{liu2018future}          & RGB                    & 67.5 \\
                    6 & AnoPred \cite{liu2018future}          & Masked RGB             & 64.8 \\
                    7 & FOL-Ensemble \cite{9712446}           & RGB + Box + Flow + Ego & 73.0 \\
                    8 & STFE \cite{zhou_spatio-temporal_2022} & RGB + Flow             & 79.3 \\
            \hline
                    % v22_2
%\rowstyle{\bfseries}9 & Our (MOVAD) & RGB ($320\times240$) &  80.13 \\
                    % v30_3
\rowstyle{\bfseries}9 & Our (MOVAD)                           & RGB ($320\times240$)   & 80.11 \\
                    % v30_2
\rowstyle{\bfseries}10 & Our (MOVAD)                          & RGB ($640\times480$)   & 82.11 \\
\end{tabular}}
	\caption{Benchmarks of Video Anomaly Detection methods on the DoTA dataset.}
	\label{tab:sota-vad-auc}
\end{table}

% TODO visualizza anche la colonna OO e VO? (VO and OO columns are not shown because they do not contain anomalous traffic participants)
\begin{table*}[ht!]
	\footnotesize
	\setlength{\tabcolsep}{3.7pt}
	\centerline{\begin{tabular}{|!l||^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|}
			Model & $ST$ & $AH$ & $LA$ & $OC$ & $TC$ & $VP$ & $VO$ & $OO$ & $UK$ & $ST*$ & $AH*$ & $LA*$ & $OC*$ & $TC*$ & $VP*$ & $VO*$ & $OO*$ & $UK*$ \\
			\hline\hline
                AnoPred \cite{liu2018future}          & 69.9 & 73.6 & 75.2 & 69.7 & 73.5 & 66.3 & N/A & N/A & N/A & 70.9 & 62.6 & 60.1 & 65.6 & 65.4 & 64.9 & 64.2 & 57.8 & N/A \\
                AnoPred \cite{liu2018future} + Mask   & 66.3 & 72.2 & 64.2 & 65.4 & 65.6 & 66.6 & N/A & N/A & N/A & 72.9 & 63.7 & 60.6 & 66.9 & 65.7 & 64.0 & 58.8 & 59.9 & N/A \\
                FOL-STD \cite{9712446}                & 67.3 & 77.4 & 71.1 & 68.6 & 69.2 & 65.1 & N/A & N/A & N/A & 75.1 & 66.2 & 66.8 & 74.1 & 72.0 & 69.7 & 63.8 & 69.2 & N/A \\
                FOL-Ensemble \cite{9712446}           & 73.3 & 81.2 & 74.0 & 73.4 & 75.1 & 70.1 & N/A & N/A & N/A & \red{77.5} & 69.8 & 68.1 & \red{76.7} & 73.9 & 71.2 & 65.2 & 69.6 & N/A \\
                % STFE gli manca la colonna VP*!
                % STFE ha dei risultati non molto chiari.
                STFE \cite{zhou_spatio-temporal_2022} & 75.2 & 84.5 & 72.1 & 77.3 & 72.8 & 71.9 & N/A & N/A & N/A & \textbf{80.6} & 65.6 & 69.9 & 76.5 & 74.2 & N/A & 75.6 & 70.5 & N/A \\
                % v22_2 epoca 370
                %\textbf{Our (MOVAD)}                  &  \red{85.6} & \red{84.5} & \red{81.4} & \red{82.8} & \red{84.9} & \textbf{85.8} & \textbf{79.1} & \red{87.4} & \textbf{76.2} & 75.1 & \red{69.9} & 68.4 & 76.6 & \red{75.8} & \red{71.8} & 74.1 & 70.2 & \red{69.4} \\
                % v30_3 560
                \textbf{Our (MOVAD)   }                &  \red{85.6} & \red{85.1} & \red{83.9} & \red{82.2} & \red{85.3} & \textbf{86.2} & \textbf{79.3} & \red{86.7} & \textbf{77.1} & 72.1 & \red{71.6} & \red{72.3} & 76.5 & \red{75.7} & \red{74.1} & \red{77.9} & \red{71.7} & \red{69.1} \\
                % v30
                %\textbf{Our (MOVAD)} &  \textbf{86.7} & \textbf{86.7} & \textbf{84.1} & \textbf{83.2} & \textbf{86.1} & \textbf{81.9} & 73.2 & \textbf{70.2} & \textbf{74.5} & \textbf{80.0} & \textbf{77.7} & \textbf{75.5} & \textbf{80.1} & \textbf{78.0}  \\
                % v30_2 (epoca 560)
                \textbf{Our (MOVAD) \dag}                &  \textbf{86.6} & \textbf{86.3} & \textbf{84.9} & \textbf{83.7} & \textbf{85.5} & \red{81.6} & \red{77.4} & \textbf{87.9} & \red{73.8} & 72.2 & \textbf{74.0} & \textbf{74.8} & \textbf{80.2} & \textbf{79.6} & \textbf{76.8} & \textbf{82.2} & \textbf{78.3} & \textbf{72.9} \\
                % v30_2 (epoca 450)
                %\textbf{Our (MOVAD) **}                  &  \textbf{87.2} & \textbf{86.6} & \textbf{85.0} & \textbf{83.8} & \textbf{86.2} & \red{79.1} & \red{77.9} & \red{87.1} & \red{74.1} & 72.8 & \textbf{73.1} & \textbf{75.7} & \textbf{82.2} & \textbf{79.0} & \textbf{74.7} & \textbf{80.3} & \textbf{77.8} & \textbf{72.1} \\
        \hline
\end{tabular}}
	\caption{Detection accuracy (AUC) for individual accident categories. "*" indicates non-ego anomaly categories. "\dag" indicates input resolution is $640\times480$ instead of $320\times240$. N/A=Not Available. Bold and red values are the best and second-best results.}
	\label{tab:sota-vad-auc-per-class}
\end{table*}

\noindent\textbf{Training details.}
Since the videos contain a non-equal number of frames, we fixed the number of frames inside the batch for each video (VCL), in order to be able to train the model with a batch size larger than one.
To make the training as diverse as possible and reduces the effect of overfitting, at each iteration, we randomly choose the starting frame for each video, adjusting the ground-truth accordingly.
Batch shape is $[B, V, H, W]$, where they are batch size, VCL, height and width, respectively.
Unless otherwise specified, the following configuration is fixed: input video is $320 \times 240$, VCL is 8, LSTM cell number is 2, $\mathit{NF}$ is 3, linear weights are initialized using a uniform distribution, the LSTM cells with a (semi-)orthogonal matrix, bias parameters are set to zero and the VST is initialized with a model pretrained on Something-Something v2~\cite{goyal2017something}.


\subsection{Ablation study}
% class weight loss
% 2xsoftmax vs 1x
% learning rate differences (sto finendo l’esperimento con multipli lr)

\noindent\textbf{Memory modules effectiveness.}
In this experiment, we study the effects of the memories (short and long).
As shown in Table \ref{tab:short-vs-long-term-auc}, both individually contribute to enhance the general performance.
As shown in row \#4, activating both memories results in the highest AUC, highlighting the need for both.

\noindent\textbf{Short-term memory module.}
% v22: numero di frames in input
In this experiment, we study the effects of the past frames to the short-term memory, varying the number of frames $\mathit{NF}$ processed by the VST at each step.
The results are displayed in Fig.~\ref{fig:num-frames-vst}.
As expected, taking into account only the current frame is the worst situation, making the training also unstable.
This is reasonable, as the short-term memory module processes and embeds only the current frame, without any knowledge of the near past, leading the long-term memory module to overfit data since consecutive frames are very similar.
Up to 4, increasing the number of frames generally increase the performance.
With 5 frames the effect becomes counterproductive.
Overall, the highest AUC is obtained with 4 frames.
% (no) v23: rand frame order (v17) vs normal
% v29_2, v29, v22: training from scratch vs pretrained (imagenet vs smth2smthv2)

\noindent\textbf{Long-term memory module.}
% posizione dell'lstm senza / prima / dopo / prim + dopo / gru
% v24_4: senza lstm
% v24, v22, v24_2: 1/2/3 # celle lstm
% v26, v26_2, v26_3: 1/2/3 # celle gru
% (no) v27, v27_2: pre + post lstm (1 cell) + saliency (?), pre + post lstm (1 cell)
In this experiment, we evaluate the long-term memory effect on the classification capability, varying the number of cells from zero (no LSTM at all) to four.
The results are displayed in Fig.~\ref{fig:num-memory-cells}.
We tested also Gated Recurrent Unit (GRU) module instead of LSTM.
Because the trend is very similar to LSTM, to make the figure easier to read, the GRU results are not shown.
The effect introduced by the memory cells is evident.
Having no cells makes training slower in saturating performance and it reaches the lowest AUC.
In general, by increasing the number of cells, its maximum AUC is reached slower in time but higher than w/out LSTM.
With 1 cell the performance saturates very quickly, with a slow degradation during the rest of the epochs.
With 4 cells leads to a very slow saturation w/out reaching the best performance.
The global maximum AUC is reached by 2 cells, even if with just +$0.02$ compared to 3 cells.
Despite this, we prefer the latter configuration because we think its ability to increase the quality of training in a slower (but not too much) and more continuous way has a better general benefit.
We verified that, in conjunction with the use of $\mathit{NF}=4$ (best configuration found in the previous experiment), it permits to obtain higher AUC than with 2 cells.
We speculate this happen because both (3 cells in Fig. \ref{fig:num-memory-cells} and $\mathit{NF}=4$ in Fig. \ref{fig:num-frames-vst}) reach the best AUC at same time (around 400 epochs).

%\noindent\textbf{Saliency module.}
% v25, v22: con/senza/versione ridotta della saliency
%In this experiment, we evaluate the effect of the saliency branch.

\noindent\textbf{Video clip length (VCL).}
% v22, v28: random_batch 4/8/12/16/20/24: describi la modalità di addestramento -> per usare un batch size > 1 si è scelto di selezionare un numero max di frame da elaborare a ogni iterazione. per aggiungere diversità al training, il punto di inizio per ogni video viene scelto in modo casuale a ogni iterazione, adattando di conseguenza il ground-truth
In this experiment, we evaluate the effect of the VCL (see training details paragraph), varying from 4 to 16.
In Fig.~\ref{fig:random-batch}, the results are shown.
The worst and most unstable training is obtained with 4 frames, probably because they are too few to exploit the long-term memory effect of LSTM cells.
Increasing the clip length permits to saturate performance quicker, but a value too high (like 12 or 16) tends to produce a lower AUC overall.
In fact, the highest AUC is obtained using 8 frames as VCL, which represents a good trade-off between enlarging clip size and exploiting LSTM cells while, at the same time, avoiding overfitting due to consecutive frames being too similar to each other.

\subsection{Comparison with the state of the art}
% input shape
% versione finale vs resto del mondo su dota, and: 
%   - Phantom: https://paperswithcode.com/paper/approaches-toward-physical-and-general-video
%   - ShanghaiTech: https://paperswithcode.com/sota/anomaly-detection-on-shanghaitech
%   - CUHK Avenue: https://paperswithcode.com/sota/anomaly-detection-on-chuk-avenue
%   - UCSD Ped2: https://paperswithcode.com/sota/abnormal-event-detection-in-video-on-ucsd
Finally, in Table \ref{tab:sota-vad-auc} we compare MOVAD (with $320\times240$ and $640\times480$ input videos size) with state-of-the-art models.
Both trained with the best configuration of: VCL of 8, 3 LSTM cells and $\mathit{NF}=4$.
Table \ref{tab:sota-vad-auc-per-class} shows results per class (see \cite{9712446} for explanation of classes).
Both MOVAD surpass the state-of-the-art (SOTA) models.
Our best MOVAD surpasses SOTA by +$2.81$ AUC, reaching the highest value of $82.11\%$ AUC.
%VB: Potrebbe servire come confronto per il numero parametri e flops
%sembra non riesca a calcolare i flops della rnn però
%| module                    | #parameters or shape   | #flops     |
%|:--------------------------|:-----------------------|:-----------|
%| model                     | 0.144G                 | 0.208T     |
%|  rnn                      |  16.794M               |  0         |
%|   rnn.weight_ih_l0        |   (4096, 1024)         |            |
%|   rnn.weight_hh_l0        |   (4096, 1024)         |            |
%|   rnn.bias_ih_l0          |   (4096,)              |            |
%|   rnn.bias_hh_l0          |   (4096,)              |            |
%|   rnn.weight_ih_l1        |   (4096, 1024)         |            |
%|   rnn.weight_hh_l1        |   (4096, 1024)         |            |
%|   rnn.bias_ih_l1          |   (4096,)              |            |
%|   rnn.bias_hh_l1          |   (4096,)              |            |
%|  rnn_bn                   |  2.048K                |  5.12K     |
%|   rnn_bn.weight           |   (1024,)              |            |
%|   rnn_bn.bias             |   (1024,)              |            |
%|  lin1                     |  37.75M                |  37.749M   |
%|   lin1.weight             |   (1024, 36864)        |            |
%|   lin1.bias               |   (1024,)              |            |
%|  lin2                     |  1.05M                 |  1.049M    |
%|   lin2.weight             |   (1024, 1024)         |            |
%|   lin2.bias               |   (1024,)              |            |
%|  lin3                     |  2.05K                 |  2.048K    |
%|   lin3.weight             |   (2, 1024)            |            |
%|   lin3.bias               |   (2,)                 |            |
%|  bn                       |  73.728K               |  0.184M    |
%|   bn.weight               |   (36864,)             |            |
%|   bn.bias                 |   (36864,)             |            |
%|  model                    |  88.656M               |  0.208T    |
%|   model.patch_embed       |   12.672K              |   0.496G   |
%|    model.patch_embed.proj |    12.416K             |    0.472G  |
%|    model.patch_embed.norm |    0.256K              |    24.576M |
%|   model.layers            |   88.641M              |   0.208T   |
%|    model.layers.0         |    0.571M              |    18.801G |
%|    model.layers.1         |    2.19M               |    17.997G |
%|    model.layers.2         |    60.353M             |    0.153T  |
%|    model.layers.3.blocks  |    25.528M             |    17.831G |
%|   model.norm              |   2.048K               |   3.072M   |
%|    model.norm.weight      |    (1024,)             |            |
%|    model.norm.bias        |    (1024,)             |            |