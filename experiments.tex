\section{Experiments}
\label{sec:experiments}

\begin{figure}[t]
\centering
	\includegraphics[trim=0 0 0 0, clip, width=1.\linewidth]{images/exp_1.jpg}
	\caption{Performance comparison changing the number of frames (from 1 to 5) in input of the Short-term memory module. NF: number of frames. \mnote{memento risolvere il crollo di NF1}}
	\label{fig:num-frames-vst}
\end{figure}

\begin{figure}[t]
\centering
	\includegraphics[trim=0 0 0 0, clip, width=1.\linewidth]{images/exp_2.jpg}
    % TODO aggiorna caption!
	\caption{Performance comparison changing the number and type of (long-term) memory cells (from 1 to 4). If not defined means no memory cell (neither following Linear) is inside the model. Row \#4 is $NF 3$ of previous experiment in Figure \ref{fig:num-frames-vst}.}
	\label{fig:num-memory-cells}
\end{figure}

% TODO check se lasciare solo la figura o aggiungere anche la tabella!
% \begin{table}[b]
% 	\footnotesize
% 	%\setlength{\tabcolsep}{1.2pt}
% 	\begin{center}
% 		\begin{tabular}{!c|^l|^c|^c|}
% 			\# & Type & \# cells & $AUC$ \\
% 			\hline\hline
% 			        % v24_4
% 			          1 &      &   & \\
% 			        % v24
% 			          2 & LSTM & 1 & \\
%                     % v26
% 			          3 & GRU  & 1 & \\
% 			        % v22
% 		       	    4 & LSTM & 2 & 79.21 \\
% 	    	        % v26_2
% 		       	    5 & GRU  & 2 &  \\
% 	       	          % v24_2
%                     6 & LSTM & 3 & \\
% 		              % v26_3
%                     7 & GRU  & 3 & \\
%                     % v24_3
%                     8 & LSTM & 4 & \\
%                     % v27_2
%                     % 8 & LSTM &
% 	\end{tabular}
% 	\end{center}
% 	\caption{Study of long-term memory effect. Type: memory cell type. If not defined means no memory cell (neither following Linear) is inside the model. Row \#4 is $NF 3$ of previous experiment in Figure \ref{fig:num-frames-vst}.}
% 	\label{tab:long-term-memory-effect}
% \end{table}

\begin{table}[b]
	\footnotesize
	%\setlength{\tabcolsep}{1.2pt}
	\begin{center}
		\begin{tabular}{!c|^r|^c|}
			\# & \# Frames &  $AUC$ \\
			\hline\hline
			        % v28
			          1 & 4 & \\
			        % v22
			          2 & 8 & 79.21 \\
                    % v28_3
			          3 & 12 & \\
			        % v28_4
		       	    4 & 16 & \\
	    	        % v28_5
		       	    5 & 20 &  \\
	       	          % v28_6
                    6 & 24 & \\
\end{tabular}
	\end{center}
	\caption{Study the effect on training of the number of frames per video. Row \#2 is NF 3 of previous experiment in Figure \ref{fig:num-frames-vst}.}
	\label{tab:random-batch}
\end{table}

\begin{table}[b]
	\footnotesize
	%\setlength{\tabcolsep}{1.2pt}
	\begin{center}
		\begin{tabular}{!c|^l|^l|^c|}
			\# & Method & Input & $AUC$ \\
			\hline\hline
	   	              1 & ConvAE & Gray & 64.3 \\
			        2 & ConvAE & Flow & 66.3 \\
                    3 & ConvLSTMAE & Gray & 53.8 \\
                    4 & ConvLSTMAE & Flow & 62.5 \\
                    5 & AnoPred & RGB & 67.5 \\
                    6 & AnoPred & Masked RGB & 64.8 \\
            \hline
                    7 & FOL-IoU & Box + Flow + Ego & 61.2 \\
                    8 & FOL-Mask & Box + Flow + Ego & 64.0 \\
                    9 & FOL-STD & Box & 66.7 \\
                    10 & FOL-STD & Box + Ego & 67.8 \\
                    11 & FOL-STD & Box + Flow & 69.1 \\
                    12 & FOL-STD & Box + Flow + Ego & 69.7 \\
                    13 & FOL-Ensemble & RGB + Box + Flow + Ego & 73.0 \\
            \hline
                    14 & Our & RGB &   \\
\end{tabular}
	\end{center}
	\caption{Benchmarks of VAD (Video Anomaly Detection) methods on the DoTA dataset.}
	\label{tab:sota-vad-auc}
\end{table}

% TODO visualizza anche la colonna OO e VO? (VO and OO columns are not shown because they do not contain anomalous traffic participants)
\begin{table*}[b]
	\footnotesize
	%\setlength{\tabcolsep}{1.2pt}
	\begin{center}
		\begin{tabular}{!l|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|}
			Model & $ST$ & $AH$ & $LA$ & $OC$ & $TC$ & $VP$ & $ST*$ & $AH*$ & $LA*$ & $OC*$ & $TC*$ & $VP*$ & $VO*$ & $OO*$ \\
			\hline\hline
                AnoPred \cite{liu2018future}        & 69.9 & 73.6 & 75.2 & 69.7 & 73.5 & 66.3 & 70.9 & 62.6 & 60.1 & 65.6 & 65.4 & 64.9 & 64.2 & 57.8 \\
                AnoPred \cite{liu2018future} + Mask & 66.3 & 72.2 & 64.2 & 65.4 & 65.6 & 66.6 & 72.9 & 63.7 & 60.6 & 66.9 & 65.7 & 64.0 & 58.8 & 59.9 \\
                FOL-STD                             & 67.3 & 77.4 & 71.1 & 68.6 & 69.2 & 65.1 & 75.1 & 66.2 & 66.8 & 74.1 & 72.0 & 69.7 & 63.8 & 69.2 \\
                FOL-Ensemble                        & 73.3 & 81.2 & 74.0 & 73.4 & 75.1 & 70.1 & 77.5 & 69.8 & 68.1 & 76.7 & 73.9 & 71.2 & 65.2 & 69.6 \\
                % STFE gli manca la colonna VP*!
                % STFE ha dei risultati non molto chiari.
                %STFE \cite{zhou2022spatio} & 75.2 & 84.5 & 72.1 & 77.3 & 72.8 & 71.9 & 80.6 & 65.6 & 69.9 & 76.5 & 74.2 & 75.6 & 70.5 \\
                  Our  &      &      &      &      &       &      &      &      &      &      &      &      &     \\
                  % TODO fix per OO
                  tmp & 85.1 & 82.5 & 82.5 & 81.4 & 83.5 & 79.3 & 72.2 & 66.7 & 69.1 & 71.1 & 73.0 & 68.4 & 74.8 & 68.9 \\
\end{tabular}
	\end{center}
	\caption{Detection accuracy for each individual accident category (AUC) on VAD task. "*" indicates non-ego anomaly categories.}
	\label{tab:sota-vad-auc-per-class}
\end{table*}

\noindent\textbf{Dataset}.
We perform our tests on the Dota dataset \cite{9712446}.
It contains 4677 videos taken from YouTube channels, with a resolution of $1024 \times 720$, annotated with information about the start and end of the anomaly, the category (10 in total) and the bounding boxes of the objects or persons involved.
The videos were recorded in different countries and with different light and weather conditions.
The dataset is split in approximately $70\%$ training and $30\%$ validation.
\lnote{li vogliamo ignorare o no?}
Because our benchmarks are related to the Task 1 \cite{9712446}, (online) frame-level Video Anomaly Detection, we ignore videos with unknown category or without objects, resulting in 1,305 test videos.
Furthermore, VO and OO columns are not shown because they do not contain anomalous traffic participants.

\noindent\textbf{Evaluation Metrics}.
To evaluate the performance of the models, we use the well-known Area Under Curve (AUC) metric.
This metric evaluate how well the model temporary-locate the anomaly in the videos.

\noindent\textbf{Implementation details.}
The results of the models with which we compare our model, are taken from the respective papers.
We perform the training on a single machine with 1 A100 GPU.
We use the Stochastic Gradient Descent (SGD) optimization algorithm with a learning rate of 0.0001, a momentum of 0.9 and batch size 8.
We decided to use SGD instead of Adam because in our experiments the latter led the training too much unstable, leading the model to diverge after a few epochs.

\noindent\textbf{Training details.}
Because the videos contain a non-uniform number of frames, to be able to fast training with batch-size major then one, we fixed the number of frames for each video taken into account.
At each iteration we chose the starting frame for each video, adjusting the ground-truth accordingly, in a way to offer to the network as diverse as possible training and reduce the effect of overfitting.
Unless otherwise specified, the model is initialized using a uniform distribution for Linear weights, with a (semi) orthogonal matrix for LSTM modules and zero for bias parameters, the VST weights are initialized with a model pretrained on Something-Something v2 and input video shape is $240 \times 320$.

\subsection{Ablation study}

% class weight loss
% 2xsoftmax vs 1x
% learning rate differences (sto finendo l’esperimento con multipli lr)

\noindent\textbf{Short-term memory module.}

% v22: numero di frames in input
In this experiment, we empirically show the effect of the input frames to the Short-term memory module, varying the number of frames processed by the VST at each step.
The results are displayed in the Figure \ref{fig:num-frames-vst}.
As we expected, taking into account only the current frame is the worst situation, because an anomaly can only be recognized by processing a wider time frame.
With 4 frames we have the best result.
Increasing the number of frames processed at each step has a much more limited effect.
While, already with 5 frames the effect become counterproductive.

% v23: rand frame order (v17) vs normal
% v29_2, v29, v22: training from scratch vs pretrained (imagenet vs smth2smthv2)

\noindent\textbf{Long-term memory module.}
% posizione dell'lstm senza / prima / dopo / prim + dopo / gru
% v24_4: senza lstm
% v24, v22, v24_2: 1/2/3 # celle lstm
% v26, v26_2, v26_3: 1/2/3 # celle gru
% v27, v27_2: pre + post lstm (1 cell) + saliency (?), pre + post lstm (1 cell)
In this experiment, we evaluate the long-term memory effect on the classification capability.
In Figure \ref{fig:num-memory-cells}, we compare the network with and without the long-term memory.
For the long-term memory we test the LSTM and the GRU modules, with a number of cells varying from 1 to 3.

%\noindent\textbf{Saliency module.}
% v25, v22: con/senza/versione ridotta della saliency
%In this experiment, we evaluate the effect of the saliency branch.

\noindent\textbf{Video clip length.}
% v22, v28: random_batch 4/8/12/16/20/24: describi la modalità di addestramento -> per usare un batch size > 1 si è scelto di selezionare un numero max di frame da elaborare a ogni iterazione. per aggiungere diversità al training, il punto di inizio per ogni video viene scelto in modo casuale a ogni iterazione, adattando di conseguenza il ground-truth
As mentioned at the beginning of the section, at each iteration, a random starting point of the video is selected and a fixed number of video frames is used to train the network.
In Table \ref{tab:random-batch}, we evaluate the effect of the size of the video sub-sample on training.

\noindent\textbf{XXXX model}
% input shape
% versione finale vs resto del mondo su dota, and: 
%   - Phantom: https://paperswithcode.com/paper/approaches-toward-physical-and-general-video
%   - ShanghaiTech: https://paperswithcode.com/sota/anomaly-detection-on-shanghaitech
%   - CUHK Avenue: https://paperswithcode.com/sota/anomaly-detection-on-chuk-avenue
%   - UCSD Ped2: https://paperswithcode.com/sota/abnormal-event-detection-in-video-on-ucsd
Finally, in Table \ref{tab:sota-vad-auc} we compare our architecture performance with state of the art models.
