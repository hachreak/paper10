@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})



@article{pawar2022deep,
    title={Deep learning based detection and localization of road accidents from traffic surveillance videos},
    author={Pawar, Karishma and Attar, Vahida},
    journal={ICT Express},
    volume={8},
    number={3},
    pages={379--387},
    year={2022},
    publisher={Elsevier}
}

@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@inproceedings{DBLP:conf/iclr/DosovitskiyB0WZ21,
    author    = {Alexey Dosovitskiy and
    Lucas Beyer and
    Alexander Kolesnikov and
    Dirk Weissenborn and
    Xiaohua Zhai and
    Thomas Unterthiner and
    Mostafa Dehghani and
    Matthias Minderer and
    Georg Heigold and
    Sylvain Gelly and
    Jakob Uszkoreit and
    Neil Houlsby},
    title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
    May 3-7, 2021},
    publisher = {OpenReview.net},
    year      = {2021},
    url       = {https://openreview.net/forum?id=YicbFdNTTy},
    timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
    biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2021Swin,
    title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
    author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2021}
}

@inproceedings{liu_video_2022,
    address = {New Orleans, LA, USA},
    title = {Video {Swin} {Transformer}},
    isbn = {978-1-66546-946-3},
    url = {https://ieeexplore.ieee.org/document/9878941/},
    doi = {10.1109/CVPR52688.2022.00320},
    abstract = {The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 85.9 top-1 accuracy on Kinetics-600 with ∼20× less pre-training data and ∼3× smaller model size) and temporal modeling (69.6 top-1 accuracy on SomethingSomething v2).},
    language = {en},
    urldate = {2022-10-03},
    booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
    publisher = {IEEE},
    author = {Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
    month = jun,
    year = {2022},
    pages = {3192--3201},
}

@ARTICLE{9712446,
    author={Yao, Yu and Wang, Xizi and Xu, Mingze and Pu, Zelin and Wang, Yuchen and Atkins, Ella and Crandall, David},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    title={DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos},
    year={2022},
    volume={},
    number={},
    pages={1-1},
    doi={10.1109/TPAMI.2022.3150763}}

@misc{sun_anomaly_2021,
    title = {Anomaly {Crossing}: {A} {New} {Method} for {Video} {Anomaly} {Detection} as {Cross}-domain {Few}-shot {Learning}},
    shorttitle = {Anomaly {Crossing}},
    url = {http://arxiv.org/abs/2112.06320},
    abstract = {Video anomaly detection aims to identify abnormal events that occurred in videos. Since anomalous events are relatively rare, it is not feasible to collect a balanced dataset and train a binary classiﬁer to solve the task. Thus, most previous approaches learn only from normal videos using unsupervised or semi-supervised methods. Obviously, they are limited in capturing and utilizing discriminative abnormal characteristics, which leads to compromised anomaly detection performance. In this paper, to address this issue, we propose a new learning paradigm by making full use of both normal and abnormal videos for video anomaly detection. In particular, we formulate a new learning task: cross-domain few-shot anomaly detection, which can transfer knowledge learned from numerous videos in the source domain to help solve few-shot abnormality detection in the target domain. Concretely, we leverage self-supervised training on the target normal videos to reduce the domain gap and devise a meta context perception module to explore the video context of the event in the few-shot setting. Our experiments show that our method signiﬁcantly outperforms baseline methods on DoTA and UCF-Crime datasets, and the new task contributes to a more practical training paradigm for anomaly detection.},
    language = {en},
    urldate = {2022-08-26},
    publisher = {arXiv},
    author = {Sun, Guangyu and Liu, Zhang and Wen, Lianggong and Shi, Jing and Xu, Chenliang},
    month = dec,
    year = {2021},
    note = {arXiv:2112.06320 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{pawar_deep_2022,
    title = {Deep learning based detection and localization of road accidents from traffic surveillance videos},
    volume = {8},
    issn = {2405-9595},
    url = {https://www.sciencedirect.com/science/article/pii/S2405959521001478},
    doi = {10.1016/j.icte.2021.11.004},
    abstract = {Real-world traffic surveillance videos need continuous supervision to monitor and take appropriate actions in case of fatal accidents. However, continuously monitoring them with human supervision is error prone and tedious. Therefore, a deep learning approach for automatic detection and localization of road accidents has been proposed by formulating the problem as anomaly detection. The method follows one-class classification approach and applies spatio-temporal autoencoder and sequence-to-sequence long short-term memory autoencoder for modeling spatial and temporal representations in the video. The model is executed on a real-world video traffic surveillance datasets and significant results have been achieved both qualitatively and quantitatively.},
    language = {en},
    number = {3},
    urldate = {2022-10-24},
    journal = {ICT Express},
    author = {Pawar, Karishma and Attar, Vahida},
    month = sep,
    year = {2022},
    keywords = {Deep learning, Video surveillance, Accident detection, One-class classification},
    pages = {379--387},
}

@article{zhou_spatio-temporal_2022,
    title = {Spatio-{Temporal} {Feature} {Encoding} for {Traffic} {Accident} {Detection} in {VANET} {Environment}},
    volume = {23},
    issn = {1558-0016},
    doi = {10.1109/TITS.2022.3147826},
    abstract = {In the Vehicular Ad hoc Networks (VANET) environment, recognizing traffic accident events in the driving videos captured by vehicle-mounted cameras is an essential task. Generally, traffic accidents have a short duration in driving videos, and the backgrounds of driving videos are dynamic and complex. These make traffic accident detection quite challenging. To effectively and efficiently detect accidents from the driving videos, we propose an accident detection approach based on spatio–temporal feature encoding with a multilayer neural network. Specifically, the multilayer neural network is used to encode the temporal features of video for clustering the video frames. From the obtained frame clusters, we detect the border frames as the potential accident frames. Then, we capture and encode the spatial relationships of the objects detected from these potential accident frames to confirm whether these frames are accident frames. The extensive experiments demonstrate that the proposed approach achieves promising detection accuracy and efficiency for traffic accident detection, and meets the real-time detection requirement in the VANET environment.},
    number = {10},
    journal = {IEEE Transactions on Intelligent Transportation Systems},
    author = {Zhou, Zhili and Dong, Xiaohua and Li, Zhetao and Yu, Keping and Ding, Chun and Yang, Yimin},
    month = oct,
    year = {2022},
    keywords = {Anomaly detection, Feature extraction, Videos, Real-time systems, Accidents, Encoding, Neural network, security communication, traffic accident detection, traffic safety, VANETs, Vehicular ad hoc networks},
    pages = {19772--19781},
}

@article{ruff_deep_nodate,
    title = {Deep {One}-{Class} {Classification}},
    abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
    language = {en},
    author = {Ruff, Lukas and Vandermeulen, Robert A and Görnitz, Nico and Deecke, Lucas and Siddiqui, Shoaib A and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
    pages = {10},
}

@misc{liu_feature_2020,
    title = {Feature {Transformation} {Ensemble} {Model} with {Batch} {Spectral} {Regularization} for {Cross}-{Domain} {Few}-{Shot} {Classification}},
    url = {http://arxiv.org/abs/2005.08463},
    abstract = {In this paper, we propose a feature transformation ensemble model with batch spectral regularization for the Crossdomain few-shot learning (CD-FSL) challenge. Speciﬁcally, we proposes to construct an ensemble prediction model by performing diverse feature transformations after a feature extraction network. On each branch prediction network of the model we use a batch spectral regularization term to suppress the singular values of the feature matrix during pre-training to improve the generalization ability of the model. The proposed model can then be ﬁne tuned in the target domain to address few-shot classiﬁcation. We also further apply label propagation, entropy minimization and data augmentation to mitigate the shortage of labeled data in target domains. Experiments are conducted on a number of CD-FSL benchmark tasks with four target domains and the results demonstrate the superiority of our proposed model.},
    language = {en},
    urldate = {2022-10-24},
    publisher = {arXiv},
    author = {Liu, Bingyu and Zhao, Zhen and Li, Zhenpeng and Jiang, Jianan and Guo, Yuhong and Ye, Jieping},
    month = may,
    year = {2020},
    note = {arXiv:2005.08463 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{xian_generalized_2020,
    address = {Cham},
    series = {Lecture {Notes} in {Computer} {Science}},
    title = {Generalized {Many}-{Way} {Few}-{Shot} {Video} {Classification}},
    isbn = {978-3-030-65414-6},
    doi = {10.1007/978-3-030-65414-6_10},
    abstract = {Few-shot learning methods operate in low data regimes. The aim is to learn with few training examples per class. Although significant progress has been made in few-shot image classification, few-shot video recognition is relatively unexplored and methods based on 2D CNNs are unable to learn temporal information. In this work we thus develop a simple 3D CNN baseline, surpassing existing methods by a large margin. To circumvent the need of labeled examples, we propose to leverage weakly-labeled videos from a large dataset using tag retrieval followed by selecting the best clips with visual similarities, yielding further improvement. Our results saturate current 5-way benchmarks for few-shot video classification and therefore we propose a new challenging benchmark involving more classes and a mixture of classes with varying supervision.},
    language = {en},
    booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
    publisher = {Springer International Publishing},
    author = {Xian, Yongqin and Korbar, Bruno and Douze, Matthijs and Schiele, Bernt and Akata, Zeynep and Torresani, Lorenzo},
    editor = {Bartoli, Adrien and Fusiello, Andrea},
    year = {2020},
    pages = {111--127},
}

@article{canizo_multi-head_2019,
    title = {Multi-head {CNN}–{RNN} for multi-time series anomaly detection: {An} industrial case study},
    volume = {363},
    issn = {0925-2312},
    shorttitle = {Multi-head {CNN}–{RNN} for multi-time series anomaly detection},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231219309877},
    doi = {10.1016/j.neucom.2019.07.034},
    abstract = {Detecting anomalies in time series data is becoming mainstream in a wide variety of industrial applications in which sensors monitor expensive machinery. The complexity of this task increases when multiple heterogeneous sensors provide information of different nature, scales and frequencies from the same machine. Traditionally, machine learning techniques require a separate data pre-processing before training, which tends to be very time-consuming and often requires domain knowledge. Recent deep learning approaches have shown to perform well on raw time series data, eliminating the need for pre-processing. In this work, we propose a deep learning based approach for supervised multi-time series anomaly detection that combines a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) in different ways. Unlike other approaches, we use independent CNNs, so-called convolutional heads, to deal with anomaly detection in multi-sensor systems. We address each sensor individually avoiding the need for data pre-processing and allowing for a more tailored architecture for each type of sensor. We refer to this architecture as Multi-head CNN–RNN. The proposed architecture is assessed against a real industrial case study, provided by an industrial partner, where a service elevator is monitored. Within this case study, three type of anomalies are considered: point, context-specific, and collective.The experimental results show that the proposed architecture is suitable for multi-time series anomaly detection as it obtained promising results on the real industrial scenario.},
    language = {en},
    urldate = {2022-10-24},
    journal = {Neurocomputing},
    author = {Canizo, Mikel and Triguero, Isaac and Conde, Angel and Onieva, Enrique},
    month = oct,
    year = {2019},
    keywords = {Anomaly detection, Deep learning, Convolutional neural networks, Industry 4.0, Multi-sensor systems, Recurrent neural networks},
    pages = {246--260},
}

@inproceedings{hasan2016learning,
    title={Learning temporal regularity in video sequences},
    author={Hasan, Mahmudul and Choi, Jonghyun and Neumann, Jan and Roy-Chowdhury, Amit K and Davis, Larry S},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={733--742},
    year={2016}
}

@inproceedings{wang2018abnormal,
    title={Abnormal event detection in videos using hybrid spatio-temporal autoencoder},
    author={Wang, Lin and Zhou, Fuqiang and Li, Zuoxin and Zuo, Wangxia and Tan, Haishu},
    booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
    pages={2276--2280},
    year={2018},
    organization={IEEE}
}

@inproceedings{luo2017remembering,
    title={Remembering history with convolutional lstm for anomaly detection},
    author={Luo, Weixin and Liu, Wen and Gao, Shenghua},
    booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)},
    pages={439--444},
    year={2017},
    organization={IEEE}
}

@inproceedings{liu2018future,
    title={Future frame prediction for anomaly detection--a new baseline},
    author={Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={6536--6545},
    year={2018}
}

@inproceedings{yao2019unsupervised,
    title={Unsupervised traffic accident detection in first-person videos},
    author={Yao, Yu and Xu, Mingze and Wang, Yuchen and Crandall, David J and Atkins, Ella M},
    booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    pages={273--280},
    year={2019},
    organization={IEEE}
}

@inproceedings{xu2019temporal,
    title={Temporal recurrent networks for online action detection},
    author={Xu, Mingze and Gao, Mingfei and Chen, Yi-Ting and Davis, Larry S and Crandall, David J},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={5532--5541},
    year={2019}
}

@misc{fang2019dada,
	doi = {10.48550/ARXIV.1912.12148},
	url = {https://arxiv.org/abs/1912.12148},
	author = {Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {DADA: A Large-scale Benchmark and Model for Driver Attention Prediction in Accidental Scenarios},
	publisher = {arXiv},
	year = {2019},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yao2020when,
	doi = {10.48550/ARXIV.2004.03044},
	url = {https://arxiv.org/abs/2004.03044},
	author = {Yao, Yu and Wang, Xizi and Xu, Mingze and Pu, Zelin and Atkins, Ella and Crandall, David},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {When, Where, and What? A New Dataset for Anomaly Detection in Driving Videos},
	publisher = {arXiv},
	year = {2020},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cornia2016saliency,
	doi = {10.48550/ARXIV.1609.01064},
	url = {https://arxiv.org/abs/1609.01064},
	author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {A Deep Multi-Level Network for Saliency Prediction},
	publisher = {arXiv},
	year = {2016},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lund2009riskperception,
	title = {Cross-cultural comparisons of traffic safety, risk perception, attitudes and behaviour},
	journal = {Safety Science},
	volume = {47},
	number = {4},
	pages = {547-553},
	year = {2009},
	note = {Process Safety Indicators / SRAE 2006},
	issn = {0925-7535},
	doi = {https://doi.org/10.1016/j.ssci.2008.07.008},
	url = {https://www.sciencedirect.com/science/article/pii/S0925753508001094},
	author = {Ingunn Olea Lund and Torbjörn Rundmo},
	keywords = {Traffic safety, Attitudes, Risk perception, Cross-cultural comparison},
	abstract = {The core aim of the present study is to examine cultural differences in risk perception and attitudes towards traffic safety and risk, taking behaviour in the Norwegian and the Ghanaian public. An additional aim is to discuss the applicability of various traffic measures, suited for low and middle income countries in Africa. Sample: The results of the present study are based on two self-completion questionnaire surveys carried out in February and March 2006. The first was a representative sample of the Norwegian public above 18 years of age (N=247). The second was a stratified sample of Ghanaian respondents (N=299). In Ghana the data was collected in Accra and Cape Coast. The results showed that there is potential for further improvement of safety attitudes and risk behaviour among Ghanaians as well as Norwegians. There were also differences in the respondents’ evaluation of attitudes, risk perception and behaviour. Perceived risk and attitudes also significantly predicted risk behaviour and accidents/collisions. The implications of these results for traffic safety will be discussed.}
}

@INPROCEEDINGS{yan2013hierachical,
	author={Yan, Qiong and Xu, Li and Shi, Jianping and Jia, Jiaya},
	booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
	title={Hierarchical Saliency Detection}, 
	year={2013},
	volume={},
	number={},
	pages={1155-1162},
	doi={10.1109/CVPR.2013.153}}

@INPROCEEDINGS{jiang2015salicon,
	author={Jiang, Ming and Huang, Shengsheng and Duan, Juanyong and Zhao, Qi},
	booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={SALICON: Saliency in Context}, 
	year={2015},
	volume={},
	number={},
	pages={1072-1080},
	doi={10.1109/CVPR.2015.7298710}}
	
@inproceedings{bao2021drive,
  title={DRIVE: Deep reinforced accident anticipation with visual explanation},
  author={Bao, Wentao and Yu, Qi and Kong, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7619--7628},
  year={2021}
}

@inproceedings{cornia2016deep,
  title={A deep multi-level network for saliency prediction},
  author={Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
  pages={3488--3493},
  year={2016},
  organization={IEEE}
}

@inproceedings{codevilla2019exploring,
  title={Exploring the limitations of behavior cloning for autonomous driving},
  author={Codevilla, Felipe and Santana, Eder and L{\'o}pez, Antonio M and Gaidon, Adrien},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9329--9338},
  year={2019}
}

@ARTICLE{4298901,
  author={Gandhi, Tarak and Trivedi, Mohan Manubhai},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Pedestrian Protection Systems: Issues, Survey, and Challenges}, 
  year={2007},
  volume={8},
  number={3},
  pages={413-430},
  doi={10.1109/TITS.2007.903444}}
  
@article{fatemidokht2021efficient,
  title={Efficient and secure routing protocol based on artificial intelligence algorithms with UAV-assisted for vehicular ad hoc networks in intelligent transportation systems},
  author={Fatemidokht, Hamideh and Rafsanjani, Marjan Kuchaki and Gupta, Brij B and Hsu, Ching-Hsien},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={7},
  pages={4757--4769},
  year={2021},
  publisher={IEEE}
}

@article{xu2021long,
  title={Long short-term transformer for online action detection},
  author={Xu, Mingze and Xiong, Yuanjun and Chen, Hao and Li, Xinyu and Xia, Wei and Tu, Zhuowen and Soatto, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1086--1099},
  year={2021}
}

@InProceedings{Arnab_2021_ICCV,
    author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu\v{c}i\'c, Mario and Schmid, Cordelia},
    title     = {ViViT: A Video Vision Transformer},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {6836-6846}
}


@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@article{gru,
  author    = {KyungHyun Cho and
               Bart van Merrienboer and
               Dzmitry Bahdanau and
               Yoshua Bengio},
  title     = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  journal   = {CoRR},
  volume    = {abs/1409.1259},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1259},
  eprinttype = {arXiv},
  eprint    = {1409.1259},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMBB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chong2017abnormal,
  title={Abnormal event detection in videos using spatiotemporal autoencoder},
  author={Chong, Yong Shean and Tay, Yong Haur},
  booktitle={Advances in Neural Networks-ISNN 2017: 14th International Symposium, ISNN 2017, Sapporo, Hakodate, and Muroran, Hokkaido, Japan, June 21--26, 2017, Proceedings, Part II 14},
  pages={189--196},
  year={2017},
  organization={Springer}
}

@misc{chung2014empirical,
  abstract = {In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.},
  added-at = {2017-11-10T14:34:25.000+0100},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/24e484a5052abe2440c0e02fd1490adfa/izzy278},
  interhash = {71c265fbc9bb1b77ff01e49d8f3d5387},
  intrahash = {4e484a5052abe2440c0e02fd1490adfa},
  keywords = {rnn units uw_ws17_ml},
  note = {cite arxiv:1412.3555Comment: Presented in NIPS 2014 Deep Learning and Representation Learning  Workshop},
  timestamp = {2017-11-10T14:34:25.000+0100},
  title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling},
  url = {http://arxiv.org/abs/1412.3555},
  year = 2014
}

@article{ramachandra2020survey,
  title={A survey of single-scene video anomaly detection},
  author={Ramachandra, Bharathkumar and Jones, Michael J and Vatsavai, Ranga Raju},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={5},
  pages={2293--2312},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wang2013action,
  title={Action recognition with improved trajectories},
  author={Wang, Heng and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3551--3558},
  year={2013}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5842--5850},
  year={2017}
}
